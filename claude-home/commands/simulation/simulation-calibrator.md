---
description: "Test and refine simulation accuracy with validation loops, bias detection, and continuous improvement frameworks."
---

## Instructions

You are tasked with systematically calibrating simulations to ensure accuracy, reliability, and actionable insights. Follow this approach: **$ARGUMENTS**

### 1. Prerequisites Assessment

**Critical Calibration Context Validation:**

- **Simulation Type**: What kind of simulation are you calibrating?
- **Accuracy Requirements**: How precise does the simulation need to be?
- **Validation Data**: What real-world data can test simulation accuracy?
- **Decision Stakes**: How important are the decisions based on this simulation?
- **Update Frequency**: How often should calibration be performed?

**If context is unclear, guide systematically:**

```
Missing Simulation Context:
"What type of simulation needs calibration?
- Business Simulations: Market response, financial projections, strategic scenarios
// ... (13 lines truncated)
```

### 2. Baseline Accuracy Assessment

**Establish current simulation performance levels:**

#### Historical Validation Framework
```
Simulation Accuracy Baseline:

Back-Testing Analysis:
// ... (18 lines truncated)
```

#### Simulation Quality Scoring
```
Quality Assessment Framework:

Input Quality (25% weight):
// ... (25 lines truncated)
```

### 3. Systematic Bias Detection

**Identify and correct simulation biases:**

#### Bias Identification Framework
```
Common Simulation Biases:

Cognitive Biases:
// ... (20 lines truncated)
```

#### Bias Mitigation Strategies
```
Systematic Bias Correction:

Process-Based Mitigation:
// ... (17 lines truncated)
```

### 4. Validation Loop Design

**Create systematic accuracy improvement processes:**

#### Multi-Level Validation Framework
```
Comprehensive Validation Approach:

Level 1: Internal Consistency Validation
// ... (23 lines truncated)
```

#### Feedback Integration Mechanisms
- Automated accuracy tracking and alert systems
- Stakeholder feedback collection and analysis
- Expert consultation and validation scheduling
- Real-world outcome monitoring and comparison

### 5. Real-Time Calibration Systems

**Establish ongoing accuracy monitoring and adjustment:**

#### Continuous Monitoring Framework
```
Real-Time Calibration Dashboard:

Accuracy Tracking Metrics:
// ... (17 lines truncated)
```

#### Adaptive Learning Integration
- Machine learning model updates based on new data
- Bayesian updating for probability and parameter estimation
- Expert feedback integration and model refinement
- Context-aware calibration for different scenario types

### 6. Calibration Quality Assurance

**Ensure systematic improvement and reliability:**

#### Calibration Validation Framework
```
Meta-Calibration Assessment:

Calibration Process Quality:
// ... (17 lines truncated)
```

#### Quality Control Mechanisms
- Independent calibration validation and audit
- Cross-functional calibration team and review processes
- External benchmark comparison and best practice integration
- Documentation and knowledge management systems

### 7. Simulation Improvement Roadmap

**Generate systematic enhancement strategies:**

#### Calibration-Based Improvement Plan
```
Simulation Enhancement Framework:

## Simulation Calibration Analysis: [Simulation Name]
// ... (47 lines truncated)
```

### 8. Knowledge Capture and Transfer

**Establish institutional learning from calibration:**

#### Learning Documentation
- Calibration methodology documentation and best practices
- Bias detection and mitigation technique libraries
- Validation approach templates and reusable frameworks
- Success pattern identification and replication guides

#### Cross-Simulation Learning
- Calibration insight sharing across different simulations
- Best practice identification and standardization
- Common pitfall documentation and avoidance strategies
- Expertise development and capability building programs

## Usage Examples

```bash
/simulation:simulation-calibrator Calibrate customer acquisition cost simulation using 12 months of actual campaign data

# Technical simulation validation
// ... (8 lines truncated)
```

## Quality Indicators

- **Green**: Systematic validation, documented biases, automated monitoring, continuous improvement
- **Yellow**: Regular validation, some bias detection, manual monitoring, periodic improvement
- **Red**: Ad-hoc validation, undetected biases, no monitoring, no improvement tracking

## Common Pitfalls to Avoid

- Validation theater: Going through validation motions without learning
- Bias blindness: Not recognizing systematic errors and prejudices
- Static calibration: Not updating models based on new information
- Perfection paralysis: Waiting for perfect accuracy before using insights
- Context ignorance: Not adapting calibration to different scenarios
- Learning isolation: Not sharing insights across teams and simulations

Transform simulation accuracy from guesswork into systematic, reliable decision support through comprehensive calibration and continuous improvement.
