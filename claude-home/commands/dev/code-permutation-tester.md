---
description: "Test multiple code variations through simulation before implementation with quality gates and performance prediction."
---

## Instructions

You are tasked with systematically testing multiple code implementation approaches through simulation to optimize decisions before actual development. Follow this approach: **$ARGUMENTS**

### 1. Prerequisites Assessment

**Critical Code Context Validation:**

- **Code Scope**: What specific code area/function/feature are you testing variations for?
- **Variation Types**: What different approaches are you considering?
- **Quality Criteria**: How will you evaluate which variation is best?
- **Constraints**: What technical, performance, or resource constraints apply?
- **Decision Timeline**: When do you need to choose an implementation approach?

**If context is unclear, guide systematically:**

```
Missing Code Scope:
"What specific code area needs permutation testing?
- Algorithm Implementation: Different algorithmic approaches for the same problem
// ... (15 lines truncated)
```

### 2. Code Variation Generation

**Systematically identify and structure implementation alternatives:**

#### Implementation Approach Matrix
```
Code Variation Framework:

Algorithmic Variations:
// ... (23 lines truncated)
```

#### Variation Specification Framework
```
For each code variation:

Implementation Details:
// ... (17 lines truncated)
```

### 3. Simulation Framework Design

**Create testing environment for code variations:**

#### Code Simulation Methodology
```
Multi-Dimensional Testing Approach:

Performance Simulation:
// ... (23 lines truncated)
```

#### Testing Environment Setup
- Isolated testing environments for each variation
- Consistent data sets and test scenarios across variations
- Automated testing pipeline and result collection
- Realistic production environment simulation

### 4. Quality Gate Framework

**Establish systematic evaluation criteria:**

#### Multi-Criteria Evaluation Matrix
```
Code Quality Assessment Framework:

Performance Gates (25% weight):
// ... (25 lines truncated)
```

#### Threshold Management
- Minimum acceptable scores for each quality dimension
- Trade-off analysis for competing quality attributes
- Conditional gates based on specific use case requirements
- Risk-adjusted thresholds for different implementation approaches

### 5. Predictive Performance Modeling

**Forecast real-world behavior before implementation:**

#### Performance Prediction Framework
```
Multi-Layer Performance Modeling:

Micro-Benchmarks:
// ... (23 lines truncated)
```

#### Confidence Interval Calculation
- Statistical analysis of performance variation across test runs
- Confidence levels for performance predictions under different conditions
- Sensitivity analysis for key performance parameters
- Risk assessment for performance-related business impacts

### 6. Risk and Trade-off Analysis

**Systematic evaluation of implementation choices:**

#### Technical Risk Assessment
```
Risk Evaluation Framework:

Implementation Risks:
// ... (17 lines truncated)
```

#### Trade-off Optimization
- Pareto frontier analysis for competing objectives
- Multi-objective optimization for quality attributes
- Scenario-based trade-off evaluation
- Stakeholder preference weighting and consensus building

### 7. Decision Matrix and Recommendations

**Generate systematic implementation guidance:**

#### Code Variation Evaluation Summary
```
## Code Permutation Analysis: [Feature/Module Name]

### Variation Comparison Matrix
// ... (35 lines truncated)
```

### 8. Continuous Learning Integration

**Establish feedback loops for approach refinement:**

#### Implementation Validation
- Real-world performance comparison to simulation predictions
- Developer experience and productivity measurement
- User feedback and satisfaction assessment
- Business outcome tracking and success evaluation

#### Knowledge Capture
- Decision rationale documentation and lessons learned
- Best practice identification and pattern library development
- Anti-pattern recognition and avoidance strategies
- Team capability building and expertise development

## Usage Examples

```bash
/dev:code-permutation-tester Test 5 different sorting algorithms for large dataset processing with memory and speed constraints

# Architecture pattern evaluation
// ... (8 lines truncated)
```

## Quality Indicators

- **Green**: Multiple variations tested, comprehensive quality gates, validated performance predictions
- **Yellow**: Some variations tested, basic quality assessment, estimated performance  
- **Red**: Single approach, minimal testing, unvalidated assumptions

## Common Pitfalls to Avoid

- Premature optimization: Over-engineering for theoretical rather than real requirements
- Analysis paralysis: Testing too many variations without making decisions
- Context ignorance: Not considering real-world constraints and team capabilities
- Quality tunnel vision: Optimizing for single dimension while ignoring others
- Simulation disconnect: Testing scenarios that don't match production reality
- Decision delay: Not acting on simulation results in timely manner

Transform code implementation from guesswork into systematic, evidence-based decision making through comprehensive variation testing and simulation.
