name: Nightly Quality Assurance

on:
  schedule:
    # Run every night at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_intensity:
        description: 'Test intensity level'
        required: false
        default: 'standard'
        type: choice
        options:
          - light
          - standard
          - intensive
      include_performance_benchmarks:
        description: 'Include performance benchmarks'
        required: false
        default: true
        type: boolean

env:
  CLAUDE_VOICE_HOME: ${{ github.workspace }}/.tmux/claude
  CI: true
  NIGHTLY_BUILD: true

jobs:
  # Extended test suite with stress testing
  extended-testing:
    name: Extended Test Suite
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      matrix:
        test-scenario:
          - stress-test
          - edge-cases
          - regression
          - compatibility
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup extended test environment
        run: |
          # Enhanced test environment for nightly builds
          mkdir -p $CLAUDE_VOICE_HOME/{logs,tests/output,cache,stress-test}
          
          # Install stress testing tools
          sudo apt-get update
          sudo apt-get install -y stress-ng parallel time valgrind strace
          
          # Setup test isolation with resource limits
          ulimit -n 1024  # File descriptor limit
          ulimit -u 512   # Process limit
          ulimit -m 1048576  # Memory limit (1GB)

      - name: Run ${{ matrix.test-scenario }} tests
        run: |
          cd .tmux/claude/tests
          
          case "${{ matrix.test-scenario }}" in
            "stress-test")
              echo "ðŸ”¥ Running stress tests..."
              
              # CPU stress test during module loading
              stress-ng --cpu 2 --timeout 30s &
              stress_pid=$!
              
              # Test module loading under stress
              for i in {1..10}; do
                echo "Stress iteration $i/10"
                timeout 30 ./test_runner.sh unit || echo "Stress test iteration $i failed"
                sleep 1
              done
              
              # Stop stress test
              kill $stress_pid 2>/dev/null || true
              ;;
              
            "edge-cases")
              echo "ðŸŽ¯ Running edge case tests..."
              
              # Test with unusual environments
              export CLAUDE_VOICE_TEST_EDGE_CASES=true
              
              # Test with limited resources
              ulimit -n 64
              timeout 600 ./test_runner.sh edge-cases 2>&1 | tee edge-cases-output.log
              ;;
              
            "regression")
              echo "ðŸ” Running regression tests..."
              
              # Test against known issues
              export CLAUDE_VOICE_TEST_REGRESSION=true
              timeout 600 ./test_runner.sh regression 2>&1 | tee regression-output.log
              ;;
              
            "compatibility")
              echo "ðŸ”„ Running compatibility tests..."
              
              # Test with different shell versions
              export CLAUDE_VOICE_TEST_COMPATIBILITY=true
              
              # Test bash versions if available
              for bash_version in /bin/bash /usr/bin/bash; do
                if [[ -x "$bash_version" ]]; then
                  echo "Testing with $bash_version"
                  $bash_version -c "source ../core/base.sh && echo 'Compatibility test passed'"
                fi
              done
              ;;
          esac

      - name: Collect detailed logs
        if: always()
        run: |
          # Collect system information
          echo "=== System Information ===" > system-info.log
          uname -a >> system-info.log
          free -h >> system-info.log
          df -h >> system-info.log
          
          # Collect process information
          echo "=== Process Information ===" >> system-info.log
          ps aux | grep -E "(claude|test)" >> system-info.log || true
          
          # Check for core dumps
          if ls core.* 1> /dev/null 2>&1; then
            echo "Core dumps found:"
            ls -la core.*
          fi

      - name: Upload extended test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: extended-test-results-${{ matrix.test-scenario }}
          path: |
            .tmux/claude/tests/*-output.log
            .tmux/claude/logs/
            system-info.log
            core.*
          retention-days: 30

  # Performance benchmarking
  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: github.event.inputs.include_performance_benchmarks != 'false'
    timeout-minutes: 30
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup benchmark environment
        run: |
          # Install benchmarking tools
          sudo apt-get update
          sudo apt-get install -y hyperfine perf-tools-unstable htop iotop
          
          # Setup CPU governor for consistent benchmarks
          echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor || true

      - name: Module loading benchmarks
        run: |
          cd .tmux/claude/core
          
          echo "ðŸ“Š Running module loading benchmarks..."
          
          # Benchmark individual module loading
          for module in *.sh; do
            if [[ -f "$module" ]]; then
              echo "Benchmarking $module..."
              hyperfine --warmup 3 --runs 10 "bash -n $module" \
                --export-json "${module%.sh}-benchmark.json" || true
            fi
          done
          
          # Benchmark full system initialization
          hyperfine --warmup 2 --runs 5 \
            "source base.sh && source module_loader.sh" \
            --export-json "system-init-benchmark.json" || true

      - name: Memory profiling
        run: |
          echo "ðŸ§  Running memory profiling..."
          
          cd .tmux/claude/core
          
          # Memory usage profiling with valgrind (if available)
          if command -v valgrind >/dev/null 2>&1; then
            echo "Running memory leak detection..."
            timeout 300 valgrind --tool=memcheck --leak-check=full \
              bash -c "source base.sh" 2> memory-profile.log || true
          fi
          
          # Simple memory monitoring
          bash -c "
            source base.sh
            source module_loader.sh
            echo 'Memory after module loading:'
            cat /proc/meminfo | head -5
          " > memory-usage.log

      - name: Function call performance
        run: |
          echo "âš¡ Benchmarking function calls..."
          
          cd .tmux/claude/tests
          
          # Create performance test script
          cat > perf-test.sh << 'EOF'
          #!/bin/bash
          source ../core/base.sh
          
          # Benchmark log function
          for i in {1..1000}; do
            log "DEBUG" "Performance test iteration $i" >/dev/null 2>&1
          done
          EOF
          
          chmod +x perf-test.sh
          
          # Benchmark the performance test
          hyperfine --warmup 1 --runs 5 "./perf-test.sh" \
            --export-json "function-call-benchmark.json" || true

      - name: Generate performance report
        run: |
          echo "ðŸ“ˆ Generating performance report..."
          
          cat > performance-report.md << 'EOF'
          # Performance Benchmark Report
          
          ## Module Loading Performance
          EOF
          
          # Process benchmark results
          find . -name "*-benchmark.json" | while read -r benchmark_file; do
            if [[ -f "$benchmark_file" ]]; then
              module_name=$(basename "$benchmark_file" .json)
              echo "### $module_name" >> performance-report.md
              
              # Extract mean time from hyperfine JSON
              mean_time=$(jq -r '.results[0].mean // "N/A"' "$benchmark_file" 2>/dev/null || echo "N/A")
              echo "- Mean execution time: ${mean_time}s" >> performance-report.md
              echo "" >> performance-report.md
            fi
          done
          
          # Add memory information
          echo "## Memory Usage" >> performance-report.md
          if [[ -f memory-usage.log ]]; then
            echo "\`\`\`" >> performance-report.md
            cat memory-usage.log >> performance-report.md
            echo "\`\`\`" >> performance-report.md
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmarks-${{ github.run_number }}
          path: |
            *-benchmark.json
            performance-report.md
            memory-*.log
          retention-days: 60

  # Security deep scan
  security-deep-scan:
    name: Deep Security Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 25
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install security tools
        run: |
          # Install additional security tools
          sudo apt-get update
          sudo apt-get install -y bandit lynis chkrootkit rkhunter
          
          # Install semgrep for advanced static analysis
          pip install semgrep

      - name: Static security analysis
        run: |
          echo "ðŸ” Running static security analysis..."
          
          # Run semgrep with security rules
          semgrep --config=auto .tmux/claude/ \
            --json --output=semgrep-results.json || true
          
          # Check for common security issues in shell scripts
          find .tmux/claude -name "*.sh" -exec grep -l "password\|secret\|key" {} \; > potential-secrets.txt || true
          
          # Check for world-writable files
          find .tmux/claude -type f -perm -002 > world-writable.txt || true

      - name: Dynamic security testing
        run: |
          echo "ðŸ›¡ï¸ Running dynamic security tests..."
          
          cd .tmux/claude/tests
          
          # Test input validation with malicious inputs
          export CLAUDE_VOICE_SECURITY_TEST=true
          timeout 300 ./test_runner.sh security-deep 2>&1 | tee security-deep-output.log || true

      - name: System security audit
        run: |
          echo "ðŸ” Running system security audit..."
          
          # Basic system hardening check
          lynis audit system --quick --quiet > lynis-audit.log 2>&1 || true
          
          # Check for rootkits (basic scan)
          chkrootkit > chkrootkit.log 2>&1 || true

      - name: Generate security report
        run: |
          echo "ðŸ“‹ Generating security report..."
          
          cat > security-report.md << 'EOF'
          # Deep Security Analysis Report
          
          ## Static Analysis Results
          EOF
          
          # Process semgrep results
          if [[ -f semgrep-results.json ]]; then
            findings=$(jq '.results | length' semgrep-results.json 2>/dev/null || echo 0)
            echo "- Security findings: $findings" >> security-report.md
            
            if [[ $findings -gt 0 ]]; then
              echo "### Critical Findings" >> security-report.md
              jq -r '.results[] | select(.extra.severity == "ERROR") | "- " + .message + " (" + .path + ":" + (.start.line | tostring) + ")"' semgrep-results.json >> security-report.md 2>/dev/null || true
            fi
          fi
          
          # Check potential secrets
          if [[ -s potential-secrets.txt ]]; then
            echo "### Potential Secrets" >> security-report.md
            echo "Files that may contain secrets:" >> security-report.md
            while read -r file; do
              echo "- $file" >> security-report.md
            done < potential-secrets.txt
          fi
          
          echo "Security analysis completed at $(date)" >> security-report.md

      - name: Upload security results
        uses: actions/upload-artifact@v4
        with:
          name: security-deep-scan-${{ github.run_number }}
          path: |
            semgrep-results.json
            security-report.md
            lynis-audit.log
            chkrootkit.log
            security-deep-output.log
          retention-days: 90

  # Code quality deep analysis
  quality-deep-analysis:
    name: Deep Quality Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 20
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install analysis tools
        run: |
          # Install additional code analysis tools
          sudo apt-get update
          sudo apt-get install -y cloc sloccount
          
          # Install advanced shell analysis tools
          npm install -g jshint eslint

      - name: Code complexity analysis
        run: |
          echo "ðŸ“Š Running code complexity analysis..."
          
          # Lines of code analysis
          cloc .tmux/claude/ --json > cloc-results.json
          
          # Detailed function analysis
          find .tmux/claude -name "*.sh" -type f | while read -r script; do
            echo "Analyzing $script..."
            
            # Count functions, lines, complexity
            awk '
            BEGIN { functions=0; total_lines=0; complex_functions=0 }
            /^[a-zA-Z_][a-zA-Z0-9_]*\(\)/ { 
              functions++; 
              func_start = NR;
              complexity = 1;
            }
            /if|while|for|case|&&|\|\|/ && functions > 0 { complexity++ }
            /^}/ && functions > 0 {
              func_length = NR - func_start;
              if (func_length > 50 || complexity > 10) complex_functions++;
            }
            END { 
              print FILENAME ": " functions " functions, " complex_functions " complex"
            }
            ' "$script"
          done > complexity-analysis.txt

      - name: Documentation coverage analysis
        run: |
          echo "ðŸ“š Analyzing documentation coverage..."
          
          find .tmux/claude/core -name "*.sh" -type f | while read -r script; do
            total_functions=$(grep -c "^[a-zA-Z_][a-zA-Z0-9_]*() *{" "$script" 2>/dev/null || echo 0)
            documented_functions=$(grep -B5 "^[a-zA-Z_][a-zA-Z0-9_]*() *{" "$script" | grep -c "^#" || echo 0)
            
            if [[ $total_functions -gt 0 ]]; then
              coverage=$((documented_functions * 100 / total_functions))
              echo "$script: $coverage% ($documented_functions/$total_functions)"
            fi
          done > documentation-coverage.txt

      - name: Maintainability metrics
        run: |
          echo "ðŸ”§ Calculating maintainability metrics..."
          
          # Calculate cyclomatic complexity
          find .tmux/claude -name "*.sh" -type f -exec grep -c "if\|while\|for\|case\|&&\||\|" {} + > cyclomatic-complexity.txt
          
          # Calculate coupling metrics (source/include dependencies)
          find .tmux/claude -name "*.sh" -type f -exec grep -c "source\|require\|load" {} + > coupling-metrics.txt
          
          # Calculate file size distribution
          find .tmux/claude -name "*.sh" -type f -exec wc -l {} + > file-sizes.txt

      - name: Generate comprehensive quality report
        run: |
          echo "ðŸ“ˆ Generating comprehensive quality report..."
          
          cat > quality-deep-report.md << 'EOF'
          # Deep Quality Analysis Report
          
          ## Code Statistics
          EOF
          
          # Process cloc results
          if [[ -f cloc-results.json ]]; then
            total_lines=$(jq -r '.SUM.code // 0' cloc-results.json)
            total_files=$(jq -r '.SUM.nFiles // 0' cloc-results.json)
            echo "- Total lines of code: $total_lines" >> quality-deep-report.md
            echo "- Total files: $total_files" >> quality-deep-report.md
          fi
          
          # Add complexity analysis
          echo "" >> quality-deep-report.md
          echo "## Complexity Analysis" >> quality-deep-report.md
          if [[ -f complexity-analysis.txt ]]; then
            echo "\`\`\`" >> quality-deep-report.md
            cat complexity-analysis.txt >> quality-deep-report.md
            echo "\`\`\`" >> quality-deep-report.md
          fi
          
          # Add documentation coverage
          echo "" >> quality-deep-report.md
          echo "## Documentation Coverage" >> quality-deep-report.md
          if [[ -f documentation-coverage.txt ]]; then
            avg_coverage=$(awk -F: '{sum+=$2} END {print sum/NR "%"}' documentation-coverage.txt || echo "N/A")
            echo "- Average documentation coverage: $avg_coverage" >> quality-deep-report.md
            echo "\`\`\`" >> quality-deep-report.md
            cat documentation-coverage.txt >> quality-deep-report.md
            echo "\`\`\`" >> quality-deep-report.md
          fi
          
          echo "Generated at: $(date)" >> quality-deep-report.md

      - name: Upload quality analysis results
        uses: actions/upload-artifact@v4
        with:
          name: quality-deep-analysis-${{ github.run_number }}
          path: |
            cloc-results.json
            quality-deep-report.md
            complexity-analysis.txt
            documentation-coverage.txt
            cyclomatic-complexity.txt
            coupling-metrics.txt
            file-sizes.txt
          retention-days: 60

  # Integration testing with external services
  integration-testing:
    name: External Integration Testing
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup mock external services
        run: |
          echo "ðŸ”Œ Setting up mock external services..."
          
          # Mock Ollama service
          docker run -d --name mock-ollama -p 11434:11434 \
            -e OLLAMA_MODELS='[{"name":"phi4-mini","size":1000000}]' \
            python:3.9-slim \
            python -c "
          import http.server
          import socketserver
          import json
          
          class MockOllamaHandler(http.server.BaseHTTPRequestHandler):
              def do_GET(self):
                  if self.path == '/api/tags':
                      response = {'models': [{'name': 'phi4-mini:latest'}]}
                  else:
                      response = {'status': 'mock'}
                  
                  self.send_response(200)
                  self.send_header('Content-type', 'application/json')
                  self.end_headers()
                  self.wfile.write(json.dumps(response).encode())
          
          with socketserver.TCPServer(('', 11434), MockOllamaHandler) as httpd:
              httpd.serve_forever()
          " &
          
          # Wait for services to start
          sleep 5

      - name: Run integration tests
        run: |
          cd .tmux/claude/tests
          
          echo "ðŸ”— Running integration tests with external services..."
          
          # Test Ollama integration
          export OLLAMA_HOST="http://localhost:11434"
          export CLAUDE_VOICE_TEST_INTEGRATION=true
          
          timeout 600 ./test_runner.sh integration-external 2>&1 | tee integration-external-output.log || true

      - name: Test service connectivity
        run: |
          echo "ðŸŒ Testing service connectivity..."
          
          # Test various network conditions
          curl -v http://localhost:11434/api/tags || echo "Ollama mock service test failed"
          
          # Test with network delays
          tc qdisc add dev lo root netem delay 100ms 2>/dev/null || echo "Network delay simulation not available"

      - name: Cleanup services
        if: always()
        run: |
          docker stop mock-ollama || true
          docker rm mock-ollama || true

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-external-results-${{ github.run_number }}
          path: |
            .tmux/claude/tests/integration-external-output.log
            .tmux/claude/logs/
          retention-days: 30

  # Nightly report generation
  nightly-report:
    name: Nightly Quality Report
    runs-on: ubuntu-latest
    needs: [extended-testing, performance-benchmarks, security-deep-scan, quality-deep-analysis, integration-testing]
    if: always()
    timeout-minutes: 10
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/

      - name: Generate comprehensive nightly report
        run: |
          echo "ðŸ“Š Generating comprehensive nightly report..."
          
          cat > nightly-report.md << 'EOF'
          # Claude Voice Integration - Nightly Quality Report
          
          ## Report Summary
          **Date**: $(date)
          **Build**: ${{ github.run_number }}
          **Commit**: ${{ github.sha }}
          
          ## Test Results Overview
          EOF
          
          # Check test results
          total_test_jobs=5
          successful_jobs=0
          
          test_jobs=(
            "extended-testing:${{ needs.extended-testing.result }}"
            "performance-benchmarks:${{ needs.performance-benchmarks.result }}"
            "security-deep-scan:${{ needs.security-deep-scan.result }}"
            "quality-deep-analysis:${{ needs.quality-deep-analysis.result }}"
            "integration-testing:${{ needs.integration-testing.result }}"
          )
          
          echo "| Test Suite | Status | Details |" >> nightly-report.md
          echo "|------------|--------|---------|" >> nightly-report.md
          
          for job_result in "${test_jobs[@]}"; do
            job_name=$(echo "$job_result" | cut -d: -f1)
            job_status=$(echo "$job_result" | cut -d: -f2)
            
            if [[ "$job_status" == "success" ]]; then
              status_icon="âœ…"
              successful_jobs=$((successful_jobs + 1))
            elif [[ "$job_status" == "skipped" ]]; then
              status_icon="â­ï¸"
            else
              status_icon="âŒ"
            fi
            
            echo "| $job_name | $status_icon $job_status | See artifacts |" >> nightly-report.md
          done
          
          success_rate=$((successful_jobs * 100 / total_test_jobs))
          
          cat >> nightly-report.md << EOF
          
          ## Quality Metrics
          - **Overall Success Rate**: $success_rate%
          - **Successful Test Suites**: $successful_jobs/$total_test_jobs
          
          ## Artifacts Generated
          EOF
          
          # List artifacts
          if [[ -d artifacts/ ]]; then
            find artifacts/ -type f -name "*.md" -o -name "*.json" -o -name "*.log" | while read -r artifact; do
              echo "- $(basename "$artifact")" >> nightly-report.md
            done
          fi
          
          cat >> nightly-report.md << 'EOF'
          
          ## Recommendations
          EOF
          
          if [[ $success_rate -ge 90 ]]; then
            echo "ðŸŽ‰ **Excellent**: All systems operating optimally" >> nightly-report.md
          elif [[ $success_rate -ge 75 ]]; then
            echo "âš ï¸ **Good**: Minor issues detected, review recommended" >> nightly-report.md
          else
            echo "ðŸš¨ **Action Required**: Multiple issues detected, immediate attention needed" >> nightly-report.md
          fi
          
          echo "Nightly report generated:"
          cat nightly-report.md

      - name: Upload nightly report
        uses: actions/upload-artifact@v4
        with:
          name: nightly-report-${{ github.run_number }}
          path: nightly-report.md
          retention-days: 365  # Keep nightly reports for a full year

      - name: Update issue if problems found
        if: needs.extended-testing.result == 'failure' || needs.security-deep-scan.result == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            const issueTitle = `Nightly Build Failure - ${new Date().toISOString().split('T')[0]}`;
            const issueBody = `
            ## Nightly Build Failure Report
            
            **Build Number**: ${{ github.run_number }}
            **Date**: ${new Date().toISOString()}
            **Commit**: ${{ github.sha }}
            
            ### Failed Jobs
            - Extended Testing: ${{ needs.extended-testing.result }}
            - Security Deep Scan: ${{ needs.security-deep-scan.result }}
            
            ### Next Steps
            1. Review the failed job logs in the artifacts
            2. Address the identified issues
            3. Re-run the nightly build to verify fixes
            
            This issue was automatically created by the nightly CI pipeline.
            `;
            
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: issueTitle,
              body: issueBody,
              labels: ['bug', 'ci-failure', 'nightly-build']
            });